---
title: "Grammy Awards Analysing"
author: "Paridhi Khandelwal"
date: "14 December 2020"
output: html_notebook
---

#### Load Packages

```{r}
library(rvest)
library(tidyverse)
library(genius)
library(plyr)
library(dplyr)
library(qdap)
library(tidytext)
library(gridExtra)
library(ggplot2)
library(methods)

```

#### Scrape and create table of records from 1980s
```{r}
# read webpage for Grammy Awards
webpage <- read_html("https://en.wikipedia.org/wiki/Grammy_Award_for_Record_of_the_Year")

# copy xpath for table of 1980s
XPATH80 <- '/html/body/div[3]/div[3]/div[5]/div[1]/table[5]'

# run the following to create table of songs from 1980s
table_1980 <- 
  webpage %>%
  html_nodes(xpath = XPATH80) %>%
  html_table(fill = TRUE)

d1980 <- table_1980[[1]]
```

#### Header that illustrates what how table should like like after scraping - should have 60 obs and 4 variables

```{r}
head(d1980)
```
#### Scrape and create table of records from 1990s
```{r}
webpage <- read_html("https://en.wikipedia.org/wiki/Grammy_Award_for_Record_of_the_Year")

# copy xpath for table of 1980s
XPATH90 <- '/html/body/div[3]/div[3]/div[5]/div[1]/table[6]'


# run the following to create table of songs from 1980s
table_1990 <- 
  webpage %>%
  html_nodes(xpath = XPATH90) %>%
  html_table(fill = TRUE)

d1990 <- table_1990[[1]]
```



```{r}
head(d1990)
```


#### Scrape and create table of records from 2000's
```{r}
webpage <- read_html("https://en.wikipedia.org/wiki/Grammy_Award_for_Record_of_the_Year")

# copy xpath for table of 1980s
XPATH00 <- '/html/body/div[3]/div[3]/div[5]/div[1]/table[7]'


# run the following to create table of songs from 1980s
table_2000 <- 
  webpage %>%
  html_nodes(xpath = XPATH00) %>%
  html_table(fill = TRUE)

d2000 <- table_2000[[1]]
```

```{r}
head(d2000)
```

#### Scrape and create table of records from 2010's
```{r}
webpage <- read_html("https://en.wikipedia.org/wiki/Grammy_Award_for_Record_of_the_Year")

# copy xpath for table of 1980s
XPATH10 <- '/html/body/div[3]/div[3]/div[5]/div[1]/table[8]'


# run the following to create table of songs from 1980s
table_2010 <- 
  webpage %>%
  html_nodes(xpath = XPATH10) %>%
  html_table(fill = TRUE)

d2010 <- table_2010[[1]]
```


```{r}
head(d2010)
```

#### Data cleaning for 1980's

##### Renames all the columns
```{r}
ROY80s<- d1980 %>%
  dplyr::rename(year = `Year[I]`,
  track = `Record`,
  artist = `Artist(s)`)
```

##### Selects year, track and artist and onmits the NA's in the data
```{r}
ROY80s<- na.omit(ROY80s)%>%
  select(year, track, artist)


```

##### Replaces [x] with " " wiht fucntion gsub
```{r}
ROY80s<-
  ROY80s%>%
  mutate(year = gsub(pattern = "(\\[\\d+\\])", replacement = "", x = year)) 
```

##### header to check results
```{r}
head(ROY80s)
```
##### Get lyrics for songs using genius package
```{r}
# get lyrics for songs 1980s using genius package
Lyrics80s <- ROY80s %>%
  add_genius(artist, track, type = "lyrics")
```


```{r}
head(Lyrics80s)
```

#### Data Cleaning 1990

##### Renames all the columns
```{r}
ROY90s<- d1990 %>%
  dplyr::rename(year = `Year[I]`,
  track = `Record`,
  artist = `Artist(s)`)
```

##### Selects year, track and artist and onmits the NA's in the data
```{r}
ROY90s<- na.omit(ROY90s)%>%
  select(year, track, artist)


```

##### Replaces [x] with " " with fucntion gsub
```{r}
ROY90s<-
  ROY90s%>%
  mutate(year = gsub(pattern = "(\\[\\d+\\])", replacement = "", x = year)) 
```

##### header to check results
```{r}
head(ROY90s)
```
##### Get lyrics for songs using genius package
```{r}
# get lyrics for songs 1990s
Lyrics90s <- ROY90s %>%
  add_genius(artist, track, type = "lyrics")
```

```{r}
head(Lyrics90s)
```



#### Cleaning for 2000's

##### Renames all the columns
```{r}
ROY00s<- d2000 %>%
  dplyr::rename(year = `Year[I]`,
  track = `Record`,
  artist = `Artist(s)`)
```

##### Selects year, track and artist and onmits the NA's in the data
```{r}
ROY00s<- na.omit(ROY00s)%>%
  select(year, track, artist)


```

##### Replaces [x] with " " with fucntion gsub
```{r}
ROY00s<-
  ROY00s%>%
  mutate(year = gsub(pattern = "(\\[\\d+\\])", replacement = "", x = year)) 
```

##### header 
```{r}
head(ROY00s)
```

##### Get lyrics for songs using genius package
```{r}
# get lyrics for songs 2000s
Lyrics00s <- ROY00s %>%
  add_genius(artist, track, type = "lyrics")


```

#### Cleaning for 2010's

##### Renames all the columns
```{r}
ROY10s<- d2010 %>%
  dplyr::rename(year = `Year[I]`,
  track = `Record`,
  artist = `Artist(s)`)
```

##### Selects year, track and artist and onmits the NA's in the data
```{r}
ROY10s<- na.omit(ROY10s)%>%
  select(year, track, artist)


```

##### Replaces [x] with " " with fucntion gsub
```{r}
ROY10s<-
  ROY10s%>%
  mutate(year = gsub(pattern = "(\\[\\d+\\])", replacement = "", x = year)) 
```


```{r}
head(ROY10s)
```
##### Get lyrics for songs using genius package
```{r}
# get lyrics for songs 2010s
Lyrics10s <- ROY10s %>%
  add_genius(artist, track, type = "lyrics")

```

##### This combines all the lyrics from all the years
```{r}
AllLyrics<-rbind(Lyrics80s,Lyrics90s,Lyrics00s, Lyrics10s)
```


##### This data frame, combines all the lyrics of all the years and divides them by decade, by adding a new column decade
```{r}
DecadesLyrics<-
  AllLyrics%>%
  mutate(year = as.numeric(year))%>%
  mutate(decade= floor(year - year %% 10))
```

##### This data frame breaks down the DecadesLyrics data frame where there were lyrics to words by decade
```{r}
DecadeVerseWords <- DecadesLyrics %>%
  unnest_tokens(word, lyric)
```


##### This data frame, groups DecadeVerseWords by track and decade and summarises it which gives us total number of words per songs by decade.
```{r}
PerTrack <- 
  DecadeVerseWords%>%
  group_by(track,decade)%>%
  mutate(decade=as.character(decade))%>%
  dplyr::summarise(total =n() )
  

```

#### Graph 1
```{r}
ggplot(PerTrack) +
 aes(x = decade, y = total, fill = decade) +
 geom_boxplot() +
 scale_fill_hue() +
 labs(x = "Decade", y = "Words per song", title = "Boxplots of Words per Grammy Nominated Song by Decade") +
 theme_minimal() +
 theme(legend.position = "none")
```
##### Graph 1 is a representation of Words per Grammy nominated song by decade. For this graph, we used the DecadeLyrics,DecadeVerseWords and PerTrack data frame. This graph shows us the difference in decads and the number of words user per song from 1980 to 2019. Step by step , we broke down sings into lyrics by decade, then we moved on to break down lyrics into words of each song by decade, and in the end, we count words per song by decade by grouping it by track and decade and summarising it to get the result.



#### Steps for Graph 2


```{r}
TopTenWords <- freq_terms(DecadesLyrics$lyric, 10, at.least=3, 
                    stopwords=tidytext::stop_words$word)
plot(TopTenWords)
```


##### Gives verse words 
```{r}
VerseWords <- DecadesLyrics %>%
  unnest_tokens(word, lyric)
```

##### Anti joins the verse words
```{r}
ft <- VerseWords %>%
  anti_join(stop_words)

```

##### This data frame removes all the extra stop words from the the dataframe and gives us the updated list of the top ten words. We did that using grepl function
```{r}
pattern<-"(ba|du|yeah|da|ya|ooh|gonna|na|uh|la|hol)"
TopTenDecade <- ft %>%
  dplyr::count(word, sort = TRUE) %>%
  filter(n >= 3) %>%
  filter(!grepl(pattern, word)) %>% 
  top_n(10) %>%
  mutate(word=reorder(word,desc(n)))

```
#### How we got to the filtered out stop words. 

##### For that we first scraped out data from wikipedia for Grammy nominated songs from 1980-2019. Then we screated a table for each one of them. To clean the data, I first renamed the columns to year, track and artist respectively. After that I omited all the "NAs" from the databse by using na.omit(), after omitting the NA's to remove the footnotes from variable year I replaced [x] with " " with function gsub. Now to get the lyrics I used the genius package to extract the lyrics. I did this process for 1980s, 1990s, 2000s and 2010s. After that I combined all the data frames using rbind. Now we wanted a column named decade with the decade year the particular year fell in. So for that I used the mutate function and named the data frame DecadeLyrics. Then I also made a data frame which unnested the lyrics into words of all the songs and named it DecadeVerseWords. After that we needed a Graph without filtering the stop words and also counting the words per song and group it by decade. I did that using group by and summarise and called teh data frame PerTrack. After that I had to take out the the top ten words regardless of the year and filter the stop_words from the list. For this I used tidytext package and removed the stop-words list from the DecadeLyrics data set.This then gave us words which filtered out the stop words, the 10 in the statement means ten words and at.least means atleast of length 3. So using these functions we got our top ten words. But even after that there were some words which were additionally removed by using grepl function in the TopTenDecade data frame.

#### Graph 2
```{r}
ggplot(TopTenDecade) +
 aes(x = word, weight = n) +
 geom_bar(fill = "#1f9e89") +
 labs(x = "Word", y = "Count", title = "Ten Most Popular Words of Grammy Nominated Songs from 1980 - 2019.") +
 theme_minimal()+
 theme(plot.title = element_text(size=15))
```
##### Graph 2 represents the Top Ten Most popular words from 1980-2019 and gives a count of how many times that word has been used .For this graph we found the top ten most popular words used and then we filtered out the stop words to get a better analysis. Surprising it is found that "love" is the most popular word of them all from 1980-2019. It tells us how the trending words from 1980-2019 have changed but some words have stayed forever.

#### Taking out Top Ten popular words for 1980s using freq_terms and stopwords and using the tidytext package.
```{r}
TopTen80s <- freq_terms(Lyrics80s$lyric, 10, at.least=3, 
                    stopwords=tidytext::stop_words$word)

```


```{r}
VerseWords80s <- Lyrics80s %>%
  unnest_tokens(word, lyric)
```


```{r}
ft80s <- VerseWords80s %>%
  anti_join(stop_words)

```

##### This data frame removes all the extra stop words from the the dataframe and gives us the updated list of the top ten words. We did that using grepl function. We also reorder the words in descending order of their count.
```{r}
pattern<-"(ba|du|yeah|da|ya|ooh|gonna|na|uh|la|hol)"
TopTen80s <- ft80s %>%
  dplyr::count(word, sort = TRUE) %>%
  filter(n >= 3) %>%
  filter(!grepl(pattern, word)) %>% 
  top_n(10) %>%
  mutate(word=reorder(word,desc(n)))

```

#### Taking out Top Ten popular words for 1990s
```{r}
TopTen90s <- freq_terms(Lyrics90s$lyric, 10, at.least=3, 
                    stopwords=tidytext::stop_words$word)
plot(TopTen90s)
```


```{r}
VerseWords90s <- Lyrics90s %>%
  unnest_tokens(word, lyric)
```


```{r}
ft90s <- VerseWords90s %>%
  anti_join(stop_words)

```

##### This data frame removes all the extra stop words from the the dataframe and gives us the updated list of the top ten words. We did that using grepl function
```{r}
pattern<-"(ba|du|yeah|da|ya|ooh|gonna|na|uh|la|hol)"
TopTen90s <- ft90s %>%
  dplyr::count(word, sort = TRUE) %>%
  filter(n >= 3) %>%
  filter(!grepl(pattern, word)) %>% 
  top_n(10) %>%
  mutate(word=reorder(word,desc(n)))

```

#### Taking out Top Ten popular words for 2000s
```{r}
TopTen00s <- freq_terms(Lyrics00s$lyric, 10, at.least=3, 
                    stopwords=tidytext::stop_words$word)
plot(TopTen00s)
```


```{r}
VerseWords00s <- Lyrics00s %>%
  unnest_tokens(word, lyric)
```


```{r}
ft00s <- VerseWords00s %>%
  anti_join(stop_words)

```

##### This data frame removes all the extra stop words from the the dataframe and gives us the updated list of the top ten words. We did that using grepl function We also reorder the words in descending order of their count.
```{r}
pattern<-"(ba|du|yeah|da|ya|ooh|gonna|na|uh|la|hol)"
TopTen00s <- ft00s %>%
  dplyr::count(word, sort = TRUE) %>%
  filter(n >= 3) %>%
  filter(!grepl(pattern, word)) %>% 
  top_n(10) %>%
  mutate(word=reorder(word,desc(n)))

```

#### Taking out Top Ten popular words for 2010s
```{r}
TopTen10s <- freq_terms(Lyrics10s$lyric, 10, at.least=3, 
                    stopwords=tidytext::stop_words$word)
plot(TopTen10s)
```


```{r}
VerseWords10s <- Lyrics10s %>%
  unnest_tokens(word, lyric)
```


```{r}
ft10s <- VerseWords10s %>%
  anti_join(stop_words)

```

##### This data frame removes all the extra stop words from the the dataframe and gives us the updated list of the top ten words. We did that using grepl function We also reorder the words in descending order of their count.
```{r}
pattern<-"(ba|du|yeah|da|ya|ooh|gonna|na|uh|la|hol)"
TopTen10s <- ft10s %>%
  dplyr::count(word, sort = TRUE) %>%
  filter(n >= 3) %>%
  filter(!grepl(pattern, word)) %>% 
  top_n(10) %>%
  mutate(word=reorder(word,desc(n)))

```


#### Steps for Graph 3

##### Made a plot for the top ten words used in 1980's and removed the stop words
```{r}
P1<-
  ggplot(TopTen80s) +
 aes(x = word, weight = n) +
 geom_bar(fill = "#0c4c8a") +
 labs(x = "Word", y = "Count", title = "1980s") +
 theme_minimal()
```

##### Made a plot for the top ten words used in 1990's and removed the stop words
```{r}
P2<-
ggplot(TopTen90s) +
 aes(x = word, weight = n) +
 geom_bar(fill = "#FFC0CB") +
 labs(x = "Word", y = "Count", title = "1990s") +
  
 theme_minimal()
```

##### Made a plot for the top ten words used in 2000's and removed the stop words
```{r}
P3<-
  ggplot(TopTen00s) +
 aes(x = word, weight = n) +
 geom_bar(fill = "#FF0000") +
 labs(x = "Word", y = "Count", title = "2000s") +
  
 theme_minimal()
```

##### Made a plot for the top ten words used in 2010's and removed the stop words
```{r}
P4<-
ggplot(TopTen00s) +
 aes(x = word, weight = n) +
 geom_bar(fill = "#00FFFF") +
 labs(x = "Word", y = "Count", title = "2010s") +
  
 theme_minimal()
```

##### Made a list of all the plots from 1980-2010
```{r}
PlotsList<- list(P1,P2,P3,P4)
```

#### Graph 3

##### Using grid.arrange from the gridExtra package
```{r}
grid.arrange(grobs = PlotsList, ncol = 2, top = "Top ten words by decade" )

```
##### Graph 3 shows us the Top ten word of each decade. To make this Graph 3, I indivually took out the top ten words of each decade, and then made a plot using ggplot for each decade. This gives the comparison and similarity of each decade with each other and the choice of words most used in that decade. It is interesting to know that slangs got started in the early 2000s with words like runnin, gotta etc To combine all the plots I used grid.arrange function to combine all the plots in a single graph.

#### Steps for Graph 4

##### This data frame inner joins the DecadeVerseWords and sentiments table from the tidytext package. it also groups by decade, word, year and sentiment.
```{r} 
Sentiments<-
  DecadeVerseWords%>%
  inner_join(sentiments, by="word")%>%
  group_by(decade, word, year, sentiment)

```

##### This data frame, makes all the 'positive' as 1 and 'negative' as 0 in the sentiment variable. This step also converts the decade as a character.
```{r}
Sentimentsnp<-
  Sentiments%>%
  mutate(sentiment= gsub(pattern = "positive", replacement = "1", x= sentiment))%>%
  mutate(sentiment= gsub(pattern = "negative", replacement = "0", x= sentiment))%>%
  mutate(decade =as.character(decade))
```

#### This data frame groups by sentiment year and secade. it also filter the data for where sentiment >0 i.e 1 and then summarises ti get the count
```{r}
SentimentYear<-Sentimentsnp%>%
  group_by(sentiment,year,decade)%>%
  filter(sentiment>0)%>%
  dplyr::summarise(count=n())
```

#### Graph 4
```{r}
ggplot(SentimentYear) +
  aes(x = year, fill = decade, weight = count) +
  geom_bar() +
  scale_fill_hue() +
  labs(x = "Year", y = "Net Sentiment", title = "Net Sentiment Score by Year") +
  theme_minimal()
```
##### Graph 4 gives us the net sentiment score by year and colors the bars respective to the decade they belong to. From the graph we can see that 2019 has the highest net sentiment score from all the years, and 2011 has the lowest net sentiment score of them all. This gives a good comparison of how positive or negative words were year from 1980-2019. For this graph I joined the sentiments table with the DecadeVerseWords and then calculated where the sentiment was positive and summarised to get a count for each year.

#### Steps for Graph 5

##### This data fram groups by decade and sentiment and filters it when sentiment =1, it also then summarises the count of all the values grouped by decade.
```{r}
SentimentDecade<-
  Sentimentsnp%>%
  group_by(sentiment, decade)%>%
  filter(sentiment>0)%>%
  dplyr::summarise(count=n())

```

##### This data frame gives us the Mean sentiment score by decade. As we know there are 10 years in a decade. I divided and floored the the new column by count/10 to get the everage. It also adds an "s" to all value int he variable decade using ifelse and grepl function.
```{r}
MeanSentimentDecade<-
  SentimentDecade%>%
  mutate(decade= ifelse(grepl("1980", decade), "1980s", 
                  ifelse(grepl("1990", decade), "1990s",
                  ifelse(grepl("2000", decade), "2000s",
                  ifelse(grepl("2010", decade), "2010s", "Other")))))%>%
mutate(mean_sentiment = count/10)

MeanSentimentDecade
```

#### Graph 5
```{r}
ggplot(MeanSentimentDecade) +
 aes(x = decade, fill = decade, weight = mean_sentiment) +
 geom_bar() +
 scale_fill_hue() +
 labs(x = "Decade", y = "Mean Sentiment Score", title = "Mean Sentiment Score by Decade") +
 theme_minimal()
```
##### Graph 5 shows us the Mean sentiment score by decade. As we can see 2010's again has the highest mean sentiment score and 1990s has the lowest mean sentiment score. This tells us the average net sentiment score of each decade. So at an average in 2010, the net sentiment score was around 65. To make this graph, I grouped by decade and sentiment and then calculated where sentiment =1 to give us the count of the sentiment score. 



##### This data frame adds "s" to all the values in the variable decade in SentimentYear
```{r}
SentimentYear1<-
  SentimentYear%>%
    mutate(decade= ifelse(grepl("1980", decade), "1980s", 
                  ifelse(grepl("1990", decade), "1990s",
                  ifelse(grepl("2000", decade), "2000s",
                  ifelse(grepl("2010", decade), "2010s", "Other")))))
  
```

#### Graph 6
##### USed geom_smooth and loess method to get the regression line
```{r}
ggplot(SentimentYear1) +
 aes(x = year, y = count,color =decade) +
 geom_point(size = 1L) +
 scale_color_hue() +
 geom_smooth( aes(group = 1), method ="loess" , formula = y ~ x, se = FALSE)+
 labs(x = "Year", y = "Net Sentiment", title = "Net Sentiment Score by Year of Grammy Nominated Records from 1980 - 2019 with Linear Model Fit") +
 theme_minimal()+
 theme(plot.title = element_text(size=10))
```

##### This graph gives us the net sentiment score by year of grammy nominated records from 1980-2019. It also gives us a linear model fit. This graph was made using gg plot and the linear model by using geom_smooth. This gave us the trend by setting a linear model fit in the graph. In this graph we colored the geom_points based on decade. Simple regression models are used to show or predict the relationship between two variables, here Year and net sentiment.




